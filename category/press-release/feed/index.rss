<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Press release &#8211; Smart Microscopy</title>
	<atom:link href="/category/press-release/feed/index.rss" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>All about imaging automation</description>
	<lastBuildDate>Thu, 18 Dec 2025 09:18:44 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	

<image>
	<url>https://i0.wp.com/smartmicroscopy.org/wp-content/uploads/2024/02/cropped-smartmicroscopy_icon-1.png?fit=32%2C32&#038;ssl=1</url>
	<title>Press release &#8211; Smart Microscopy</title>
	<link>/</link>
	<width>32</width>
	<height>32</height>
</image> 
<site xmlns="com-wordpress:feed-additions:1">228298350</site>	<item>
		<title>DySTrack: A new tool for Dynamic Sample Tracking joins the push to bring smart microscopy to everyday imaging</title>
		<link>/2025/12/18/dystrack-a-new-tool-for-dynamic-sample-tracking-joins-the-push-to-bring-smart-microscopy-to-everyday-imaging/</link>
		
		<dc:creator><![CDATA[ARates]]></dc:creator>
		<pubDate>Thu, 18 Dec 2025 09:18:43 +0000</pubDate>
				<category><![CDATA[Press release]]></category>
		<guid isPermaLink="false">/?p=3169</guid>

					<description><![CDATA[Smart microscopy promises adaptive, sample-aware imaging but has seen limited adoption in biology labs because it often requires bespoke hardware, vendor-specific solutions, or complex software frameworks. A new python-based open-source software tool, DySTrack (Dynamic Sample Tracking), now joins recent efforts to change this by making smart microscopy more accessible on standard commercial microscopes. DySTrack, developed [&#8230;]]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-columns alignwide is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<p>Smart microscopy promises adaptive, sample-aware imaging but has seen limited adoption in biology labs because it often requires bespoke hardware, vendor-specific solutions, or complex software frameworks. A new python-based open-source software tool, DySTrack (Dynamic Sample Tracking), now joins recent efforts to change this by making smart microscopy more accessible on standard commercial microscopes.</p>



<p>DySTrack, developed by Zimeng Wu, Jonas Hartmann, and colleagues at University College London, enables automated live tracking of moving samples without replacing existing acquisition software. Instead of re-engineering microscope control, DySTrack works in tandem with vendor software, allowing researchers to keep using familiar interfaces to configure acquisition settings while adding custom image analysis pipelines into the acquisition loop.</p>



<p>In a typical experiment, DySTrack continuously monitors high-speed prescans, runs user-defined python image analysis pipelines, and feeds the resulting coordinates back to the microscope to trigger stage movements and high-quality main scans. This loop allows automatic adjustment of the field of view to follow moving, drifting, or developing samples, reducing manual intervention and increasing experimental robustness.</p>



<p>DySTrack is designed to be lightweight and easy to adapt across different microscopes, avoiding vendor lock-in. Out-of-the-box integrations are provided for Zeiss LSM880 and LSM980 systems, as well as Nikon microscopes supporting JOBS, with support for further systems expected in future updates.</p>



<p>To help users get started, DySTrack includes example pipelines for several challenging biological applications, including tracking of the migrating zebrafish lateral line primordium and of the regressing chick Hensen&#8217;s node. These examples serve as templates that can be readily adapted to other organisms, samples and imaging setups.</p>



<p>DySTrack is fully open-source, with comprehensive documentation and a modular design that encourages community contributions and experimentation. The developers position it as a low-barrier entry point for researchers and facilities interested in smart microscopy, particularly those who seek flexibility without committing to custom hardware or large software ecosystems. The tool is described in a recent bioRxiv preprint, and the code is available on GitHub.</p>



<p>With DySTrack, smart microscopy moves another step closer to becoming an everyday tool in the biologist&#8217;s increasingly automated tool belt.</p>
</div>
</div>
</div>
</div>



<div class="wp-block-group has-base-color has-light-blue-background-color has-text-color has-background has-link-color wp-elements-462b5980ad7b88e11f35a0bb92851dcf" style="padding-top:var(--wp--preset--spacing--20);padding-right:var(--wp--preset--spacing--40);padding-bottom:var(--wp--preset--spacing--20);padding-left:var(--wp--preset--spacing--40)"><div class="wp-block-group__inner-container is-layout-constrained wp-container-core-group-is-layout-26f58b4f wp-block-group-is-layout-constrained">
<h4 class="wp-block-heading has-black-color has-text-color has-link-color wp-elements-cfc1229d547209fd523665ae8d1c3e35">More information:</h4>



<p><strong>The code is freely available on GitHub (<a href="https://github.com/WhoIsJack/DySTrack" target="_blank" rel="noreferrer noopener">https://github.com/WhoIsJack/DySTrack</a>) and a comprehensive documentation can be found online (<a href="https://whoisjack.github.io/DySTrack">https://whoisjack.github.io/DySTrack</a>). Community contributions via GitHub are welcome!</strong></p>



<p></p>



<p>Z. Wu, O. Voiculescu, A. Mongera, R. Mayor, M. Wong, and J. Hartmann (2025): <em>DySTrack: a modular smart microscopy tool for live tracking of dynamic samples on modern commercial microscopes</em>. bioRxiv, <a href="https://doi.org/10.64898/2025.12.02.691816">https://doi.org/10.64898/2025.12.02.691816</a>.</p>
</div></div>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">3169</post-id>	</item>
		<item>
		<title>AI agents advance atomic force microscopy automation: new benchmark reveals capabilities and critical safety concerns</title>
		<link>/2025/11/20/ai-agents-advance-atomic-force-microscopy-automation/</link>
		
		<dc:creator><![CDATA[ARates]]></dc:creator>
		<pubDate>Thu, 20 Nov 2025 09:34:28 +0000</pubDate>
				<category><![CDATA[Press release]]></category>
		<guid isPermaLink="false">/?p=3049</guid>

					<description><![CDATA[New research published in Nature Communications by Mandal et al. (Indian Institute of Technology Delhi, India) presents a comprehensive evaluation of large language model agents for automating atomic force microscopy. The researchers developed AILA (Artificially Intelligent Lab Assistant) and AFMBench to assess the potential and limitations of AI-powered laboratory automation. Atomic force microscopy (AFM) is [&#8230;]]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-columns alignwide is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<p>New research published in <em>Nature Communications</em> by Mandal et al. (Indian Institute of Technology Delhi, India) presents a comprehensive evaluation of large language model agents for automating atomic force microscopy. The researchers developed AILA (Artificially Intelligent Lab Assistant) and AFMBench to assess the potential and limitations of AI-powered laboratory automation.</p>



<p>Atomic force microscopy (AFM) is a powerful materials characterization technique with applications across nanotechnology, materials science, and biology, but it requires significant expertise to operate effectively. Traditional approaches demand manual parameter optimization, careful experimental design, and expert interpretation of results. When conducting complex experimental workflows, this expertise requirement can become a bottleneck, limiting throughput and accessibility. Ultimately, this restricts AFM’s broader adoption in research and industry.</p>



<p>The new work implements a multi-agent AI framework that can interpret natural language queries and autonomously orchestrate the complete AFM workflow—from experimental design and instrument calibration to imaging, data analysis, and decision-making. AILA employs specialized agents coordinated by a central planner: the AFM Handler Agent controls experimental operations while the Data Handler Agent manages analysis and optimization. The system is evaluated using AFMBench, a suite of 100 real-laboratory tasks covering the full spectrum of microscopy operations, providing a rigorous benchmark for AI agents in experimental contexts.</p>



<p>In the work, the researchers evaluated four leading language models (GPT-4o, GPT-3.5, Claude-3.5-sonnet, and Llama-3.3) across AFMBench and demonstrated AILA&#8217;s capabilities through five real-world experiments: automated microscope calibration through PID parameter optimization; high-resolution detection of graphene step edges at the atomic scale requiring sophisticated baseline correction; load-dependent friction measurements on highly oriented pyrolytic graphite with automated data plotting; identification and layer counting of graphene flakes on silicon substrates through autonomous feature detection; and analysis of indentation marks to infer indenter geometry. Overall, the applications prove the potential of AI agents for laboratory automation while revealing critical limitations: GPT-4o achieved 65% success on benchmark tasks but struggled with code generation errors; models excelling at materials science knowledge performed poorly in hands-on tasks, showing domain knowledge doesn’t translate to experimental capabilities; and critically, AI agents sometimes “sleepwalk”—deviating from instructions to perform unintended and potentially dangerous actions, raising serious safety concerns for autonomous laboratory deployment. The multi-agent architecture significantly outperformed single-agent approaches, though both remained sensitive to instruction phrasing. Importantly, this work establishes the first comprehensive benchmark for evaluating AI laboratory assistants and provides essential safety insights for the emerging field of self-driving laboratories.</p>



<p></p>
</div>
</div>
</div>
</div>



<div class="wp-block-group has-base-color has-light-blue-background-color has-text-color has-background has-link-color wp-elements-bfa8b8e8a5a23ff70c54517ecc657a6d" style="padding-top:var(--wp--preset--spacing--20);padding-right:var(--wp--preset--spacing--40);padding-bottom:var(--wp--preset--spacing--20);padding-left:var(--wp--preset--spacing--40)"><div class="wp-block-group__inner-container is-layout-constrained wp-container-core-group-is-layout-26f58b4f wp-block-group-is-layout-constrained">
<h4 class="wp-block-heading has-black-color has-text-color has-link-color wp-elements-cfc1229d547209fd523665ae8d1c3e35">More information:</h4>



<p>For the complete benchmark suite and code, see <a href="https://github.com/M3RG-IITD/AILA">the repository on GitHub</a>. <br>Indrajeet Mandal, <em>et al</em>. Evaluating large language model agents for automation of atomic force microscopy. Nat Comm <strong>16</strong> 9104 (2025) <a href="https://doi.org/10.1038/s41467-025-64105-7">https://doi.org/10.1038/s41467-025-64105-7</a></p>
</div></div>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">3049</post-id>	</item>
		<item>
		<title>Event-triggered MINFLUX microscopy boosts speed and precision in live-cell imaging</title>
		<link>/2025/10/02/event-triggered-minflux-microscopy-boosts-speed-and-precision-in-live-cell-imaging/</link>
		
		<dc:creator><![CDATA[ARates]]></dc:creator>
		<pubDate>Thu, 02 Oct 2025 12:13:56 +0000</pubDate>
				<category><![CDATA[Press release]]></category>
		<guid isPermaLink="false">/?p=2919</guid>

					<description><![CDATA[New research in a preprint by Alvelid et al. (Friedrich Schiller University Jena, Leibniz Institute of Photonic Technology, Jena, Germany; SciLifeLab, KTH Royal Institute of Technology, Stockholm, Sweden) presents a new method for performing smart MINFLUX microscopy. The researchers developed the method to enhance the capabilities and range of applications of MINFLUX. MINFLUX is a [&#8230;]]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-columns alignwide is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<p>New research in a preprint by Alvelid et al. (Friedrich Schiller University Jena, Leibniz Institute of Photonic Technology, Jena, Germany; SciLifeLab, KTH Royal Institute of Technology, Stockholm, Sweden) presents a new method for performing smart MINFLUX microscopy. The researchers developed the method to enhance the capabilities and range of applications of MINFLUX.</p>



<p>MINFLUX is a super-resolution microscopy method with powerful spatial and temporal resolution capabilities in tracking and imaging, but acquisition times spans minutes to hours as a single fluorophore is measured at a time. When measuring live cells, this can quickly become hindering, unless the microscopy is applied in a small region of interest. Ultimately, this limits the potential applications.</p>



<p>The new work implements event-triggered microscopy with confocal sample monitoring to detect cellular events in living cells through real-time image analysis. At the event sites, MINFLUX tracking is rapidly applied in a sub-micrometer-sized region of interest, exactly where and when it is the most beneficial. The method is controlled using a custom-written and standalone Python widget (based on the ImSwitch widget for event-triggered STED previously developed by Alvelid (<a href="https://doi.org/10.1038/s41592-022-01588-y)">https://doi.org/10.1038/s41592-022-01588-y)</a>) that automatically controls a commercial Abberior MINFLUX microscope. The widget is open source and available on GitHub and uses the <em>specpy</em> Python API to microscope control software <em>Imspector</em>, allowing a wide range of users to take advantage of the method.</p>



<p>In the work, the method was applied to look at three diverse biological applications in living cells: lipid diffusion dynamics were investigated in 2D at caveolae sites; the 3D membrane topography was rapidly measured through lipid tracking at developing endosomes following dynamin accumulation, allowing high precision geometrical measurements of the endosomal heads; and lastly the measurement of 3D membrane topographical and diffusional information simultaneously at multiple event sites allowed to follow HIV budding from an early stage as it develops after Gag accumulation. Overall, the applications prove the generalizability of the method to both follow development on the minutes scale of processes as well as catching second-scale dynamics of cellular events otherwise impossible to acquire through manual control, all while minimizing sample light exposure and maximizing useful data throughput. Importantly, budding endosomes and viruses were imaged in 3D in living cells for the first time, even in physiological conditions.</p>
</div>
</div>
</div>
</div>



<div class="wp-block-group has-base-color has-light-blue-background-color has-text-color has-background has-link-color wp-elements-b30c5f6f7a0c711b9b346ed857d76c81" style="padding-top:var(--wp--preset--spacing--20);padding-right:var(--wp--preset--spacing--40);padding-bottom:var(--wp--preset--spacing--20);padding-left:var(--wp--preset--spacing--40)"><div class="wp-block-group__inner-container is-layout-constrained wp-container-core-group-is-layout-26f58b4f wp-block-group-is-layout-constrained">
<h4 class="wp-block-heading has-black-color has-text-color has-link-color wp-elements-cfc1229d547209fd523665ae8d1c3e35">More information:</h4>



<p><strong>For more information on the widget, see the repository on GitHub: <a href="https://github.com/jonatanalvelid/etMINFLUX">https://github.com/jonatanalvelid/etMINFLUX</a>.</strong></p>



<p></p>



<p>Jonatan Alvelid, Agnes Koerfer, Christian Eggeling (2025). <em>Event-triggered MINFLUX microscopy: smart microscopy to catch and follow rare events.</em> bioRxiv, <a href="https://doi.org/10.1101/2025.08.27.672674" rel="nofollow">https://doi.org/10.1101/2025.08.27.672674</a></p>
</div></div>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">2919</post-id>	</item>
		<item>
		<title>Smart microscope captures aggregation of misfolded proteins</title>
		<link>/2025/08/29/smart-microscope-captures-aggregation-of-misfolded-proteins/</link>
		
		<dc:creator><![CDATA[ARates]]></dc:creator>
		<pubDate>Fri, 29 Aug 2025 12:25:28 +0000</pubDate>
				<category><![CDATA[Press release]]></category>
		<guid isPermaLink="false">/?p=2819</guid>

					<description><![CDATA[The accumulation of misfolded proteins in the brain is central to the progression of neurodegenerative diseases like Huntington’s, Alzheimer’s and Parkinson’s. But to the human eye, proteins that are destined to form harmful aggregates don’t look any different than normal proteins. The formation of such aggregates also tends to happen randomly and relatively rapidly – [&#8230;]]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-columns alignwide is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<p>The accumulation of misfolded proteins in the brain is central to the progression of neurodegenerative diseases like Huntington’s, Alzheimer’s and Parkinson’s. But to the human eye, proteins that are destined to form harmful aggregates don’t look any different than normal proteins. The formation of such aggregates also tends to happen randomly and relatively rapidly – on the scale of minutes. The ability to identify and characterize protein aggregates is essential for understanding and fighting neurodegenerative diseases.</p>



<p>Now, using deep learning, EPFL researchers have developed a ‘self-driving’ imaging system that leverages multiple microscopy methods to track and analyze protein aggregation in real time – and even anticipate it before it begins. In addition to maximizing imaging efficiency, the approach minimizes the use of fluorescent labels, which can alter the biophysical properties of cell samples and impede accurate analysis.</p>



<p>“This is the first time we have been able to accurately foresee the formation of these protein aggregates,” says recent EPFL PhD graduate Khalid Ibrahim. “Because their biomechanical properties are linked to diseases and the disruption of cellular function, understanding how these properties evolve throughout the aggregation process will lead to fundamental understanding essential for developing solutions.”</p>



<p>Ibrahim has published this work in <a href="https://doi.org/10.1038/s41467-025-60912-0" target="_blank" rel="noreferrer noopener"><em>Nature Communications</em></a> with Aleksandra Radenovic, head of the <a href="https://www.epfl.ch/labs/lben/">Laboratory of Nanoscale Biology</a> in the School of Engineering, and Hilal Lashuel in the School of Life Sciences, in collaboration with Carlo Bevilacqua and Robert Prevedel at the European Molecular Biology Laboratory in Heidelberg, Germany. The project is the result of a longstanding collaboration between the Lashuel and Radenovic labs that unites complementary expertise in neurodegeneration and advanced live-cell imaging technologies. “This project was born out of a motivation to build methods that reveal new biophysical insights, and it is exciting to see how this vision has now borne fruit,” Radenovic says.</p>



<p><strong>Witnessing the birth of a protein aggregate</strong></p>



<p>In their first collaborative effort, led by Ibrahim, the team developed a deep learning algorithm <a href="https://actu.epfl.ch/news/ai-unlocks-new-insights-in-neurodegenerative-disea/">that was able to detect mature protein aggregates</a> when presented with unlabeled images of living cells. The new study builds on that work with an image classification version of the algorithm that analyzes such images in real time: when this algorithm detects a mature protein aggregate, it triggers a Brillouin microscope, which analyzes scattered light to characterize the aggregates’ biomechanical properties like elasticity.</p>



<p>Normally, the slow imaging speed of a Brillouin microscope would make it a poor choice for studying rapidly evolving protein aggregates. But thanks to the EPFL team’s AI-driven approach, the Brillouin microscope is only switched on when a protein aggregate is detected, speeding up the entire process while opening new avenues in smart microscopy.</p>



<p>“This is the first publication that shows the impressive potential for self-driving systems to incorporate label-free microscopy methods, which should allow more biologists to adopt rapidly evolving smart microscopy techniques,” Ibrahim says.</p>



<p>Because the image classification algorithm only targets mature protein aggregates, the researchers still needed to go further if they wanted to catch aggregate formation in the act. For this, they developed a second deep learning algorithm and trained it on fluorescently labelled images of proteins in living cells. This ‘aggregation-onset’ detection algorithm can differentiate between near-identical images to correctly identify when aggregation will occur with 91% accuracy. Once this onset is spotted, the self-driving system again switches on Brillouin imaging to provide a never-before-seen window into protein aggregation. For the first time, the biomechanics of this process can be captured dynamically as it occurs.</p>



<p>Lashuel emphasizes that in addition to advancing smart microscopy, this work has important implications for drug discovery and precision medicine. “Label-free imaging approaches create entirely new ways to study and target small protein aggregates called toxic oligomers, which are thought to play central causative roles in neurodegeneration,” he says. “We are excited to build on these achievements and pave the way for drug development platforms that will accelerate more effective therapies for neurodegenerative diseases.”</p>
</div>
</div>
</div>
</div>



<div class="wp-block-group has-base-color has-light-blue-background-color has-text-color has-background has-link-color wp-elements-8b8bbfb038c6b567aac0ee6a3980cdd7" style="padding-top:var(--wp--preset--spacing--20);padding-right:var(--wp--preset--spacing--40);padding-bottom:var(--wp--preset--spacing--20);padding-left:var(--wp--preset--spacing--40)"><div class="wp-block-group__inner-container is-layout-constrained wp-container-core-group-is-layout-26f58b4f wp-block-group-is-layout-constrained">
<h4 class="wp-block-heading has-black-color has-text-color has-link-color wp-elements-cfc1229d547209fd523665ae8d1c3e35">More information:</h4>



<p><strong>Written by Celia Luterbacher for <a href="https://actu.epfl.ch/news" data-type="link" data-id="https://actu.epfl.ch/news">EPFL news</a>. Read the original article <a href="https://actu.epfl.ch/news/smart-microscope-captures-aggregation-of-misfold-2/" data-type="link" data-id="https://actu.epfl.ch/news/smart-microscope-captures-aggregation-of-misfold-2/">here</a>. </strong></p>



<p></p>



<p>Self-Driving Microscopy Detects the Onset of Protein Aggregation and Enables Intelligent Brillouin Imaging. Khalid A. Ibrahim, Camille Cathala, Carlo Bevilacqua, Lely Feletti, Robert Prevedel, Hilal A. Lashuel and Aleksandra Radenovic. <a href="https://doi.org/10.1038/s41467-025-60912-0 ">https://doi.org/10.1038/s41467-025-60912-0 </a></p>



<p>Ibrahim, K.A., Grußmayer, K.S., Riguet, N. et al. Label-free identification of protein aggregates using deep learning. Nat Commun 14, 7816 (2023). <a href="https://doi.org/10.1038/s41467-023-43440-7">https://doi.org/10.1038/s41467-023-43440-7</a></p>
</div></div>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">2819</post-id>	</item>
		<item>
		<title>Imaging 3D cell cultures with optical microscopy</title>
		<link>/2025/05/07/imaging-3d-cell-cultures-with-optical-microscopy/</link>
		
		<dc:creator><![CDATA[ARates]]></dc:creator>
		<pubDate>Wed, 07 May 2025 09:39:38 +0000</pubDate>
				<category><![CDATA[Press release]]></category>
		<guid isPermaLink="false">/?p=1908</guid>

					<description><![CDATA[A new review article published in Nature Methods surveys the application of optical microscopy techniques for 3D cell culture research. Led by Huai-Ching Hsieh (University of Washington), the review provides a general guide for selecting optimal microscopy methods based on specific research goals, while also highlighting technical innovations that may address common challenges. Since 3D [&#8230;]]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-columns alignwide is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<p>A new review article published in <em>Nature Methods</em> surveys the application of optical microscopy techniques for 3D cell culture research. Led by Huai-Ching Hsieh (University of Washington), the review provides a general guide for selecting optimal microscopy methods based on specific research goals, while also highlighting technical innovations that may address common challenges.</p>



<p>Since 3D cell cultures are more physiologically relevant than traditional 2D monolayers, they have become increasingly important for biomedical research. However, their added complexity, such as multi-layered and scaffold-based architectures, presents unique challenges for optical imaging. As dictated by specific biological end goals, a careful balancing of parameters is required, including spatial resolution, contrast, imaging depth, and compatibility with living samples (e.g., phototoxicity).</p>



<p>The review starts by outlining common optical microscopy approaches used for imaging advanced cell cultures, followed by a survey of major research areas where 3D cell cultures are commonly applied: developmental biology, infection biology, pharmacology, and cancer biology. These research areas provide real-world examples that allow the authors to comment on key technical factors to consider when selecting or optimizing microscopy methods.</p>



<p>Motivated by many of the imaging challenges identified earlier in the review, the authors provide a forward-looking discussion towards the end of the article on emerging areas of technical importance and development. For instance, adaptive optics and scattering mitigation techniques are being developed to improve image resolution and contrast deep within 3D cell cultures. Multiscale imaging workflows are also highlighted for time- and data-efficient interrogation of large 3D samples at high resolution. These workflows often combine both hardware and software innovations in a “smart” microscopy image-feedback paradigm. Multiplexed imaging is also featured as an important step towards understanding complex cellular compositions and interactions in 3D cell cultures. Emerging strategies include real-time unmixing of hyperspectral imaging datasets, which can potentially be used for feedback-based control.</p>



<p>Finally, the authors point to future possibilities in “computationally enhanced microscopy”. Examples include event-driven imaging with adaptive frame rates to capture cellular events that occur at sparse time points, such as cell division. Such approaches may be particularly powerful in the context of studying 3D cell cultures, where increasing levels of structural complexity necessitate smarter control of imaging parameters and data outputs to ensure efficient and impactful biological readouts.</p>



<p></p>
</div>
</div>
</div>
</div>



<div class="wp-block-group has-base-color has-light-blue-background-color has-text-color has-background has-link-color wp-elements-f5078ed59e4bda20cd54e1a36b282beb" style="padding-top:var(--wp--preset--spacing--20);padding-right:var(--wp--preset--spacing--40);padding-bottom:var(--wp--preset--spacing--20);padding-left:var(--wp--preset--spacing--40)"><div class="wp-block-group__inner-container is-layout-constrained wp-container-core-group-is-layout-26f58b4f wp-block-group-is-layout-constrained">
<h4 class="wp-block-heading has-black-color has-text-color has-link-color wp-elements-cfc1229d547209fd523665ae8d1c3e35">More information:</h4>



<p>Written by <a href="mailto:wheennihsieh@gmail.com">Huai-Ching Hsieh</a> and <a href="mailto:jonliu@uw.edu">Jonathan T.C. Liu</a></p>



<p>Hsieh, HC., Han, Q., Brenes, D. <em>et al.</em> Imaging 3D cell cultures with optical microscopy. <em>Nat Methods</em> (2025). <a href="https://doi.org/10.1038/s41592-025-02647-w">https://doi.org/10.1038/s41592-025-02647-w</a></p>



<p>Figure by ©Huai-Ching Hsieh and David Brenes</p>
</div></div>



<p></p>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">1908</post-id>	</item>
		<item>
		<title>Arkitekt: open-source workflows for smart microscopy</title>
		<link>/2024/11/14/arkitekt-open-source-workflows-for-smart-microscopy/</link>
		
		<dc:creator><![CDATA[ARates]]></dc:creator>
		<pubDate>Thu, 14 Nov 2024 16:16:11 +0000</pubDate>
				<category><![CDATA[Press release]]></category>
		<guid isPermaLink="false">/?p=1097</guid>

					<description><![CDATA[Arkitekt is an open-source middleware designed to integrate and orchestrate existing bioimage applications into interactive, real-time workflows. It supports popular tools like Napari, ImageJ, MicroManager, and deep learning frameworks such as CARE and StarDist. As a distributed system, Arkitekt enables each tool to run on the appropriate hardware—whether a local workstation or a GPU cluster—optimizing [&#8230;]]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-columns alignwide is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<p><em>Arkitekt</em> is an open-source middleware designed to integrate and orchestrate existing bioimage applications into interactive, real-time workflows. It supports popular tools like Napari, ImageJ, MicroManager, and deep learning frameworks such as CARE and StarDist. As a distributed system, <em>Arkitekt</em> enables each tool to run on the appropriate hardware—whether a local workstation or a GPU cluster—optimizing performance across various lab environments. It also leverages Docker containers to ensure reproducibility in running these applications.</p>



<p>The system facilitates real-time workflows, enabling &#8220;smart microscopy&#8221; by synchronizing data analysis and acquisition. Centralized data management simplifies the exploration and analysis of large datasets, helping researchers streamline complex bioimage workflows.</p>



<p>Although <em>Arkitekt</em> is still in an experimental phase and undergoing significant refactoring to enhance usability in its next iteration, it aims to provide a strong, scalable framework for the community to build upon. If you’re interested in learning more or want to share your needs for a smarter microscopy platform, visit the website or join our Discord community for updates and discussions<strong>.</strong></p>
</div>
</div>
</div>
</div>



<div class="wp-block-group has-base-color has-light-blue-background-color has-text-color has-background has-link-color wp-elements-78f2b539d9a9cba0db8aa54f75c309a5" style="padding-top:var(--wp--preset--spacing--20);padding-right:var(--wp--preset--spacing--40);padding-bottom:var(--wp--preset--spacing--20);padding-left:var(--wp--preset--spacing--40)"><div class="wp-block-group__inner-container is-layout-constrained wp-container-core-group-is-layout-26f58b4f wp-block-group-is-layout-constrained">
<h4 class="wp-block-heading has-black-color has-text-color has-link-color wp-elements-cfc1229d547209fd523665ae8d1c3e35">More information:</h4>



<p>Get more information on the <a href="https://arkitekt.live/">Akitekt website</a>!</p>



<p>If you want to learn and discuss about Arkitekt, <a href="https://discord.com/invite/pT8kggfFBk">join the Discord community</a>!</p>



<p>Johannes Roos, Stéphane Bancelin, Tom Delaire, Alexander Wilhelmi, Florian Levet, Maren Engelhardt, Virgile Viasnoff, Rémi Galland, U. Valentin Nägerl and Jean-Baptiste Sibarita (2024). <em>Arkitekt: streaming analysis and real-time workflows for microscopy</em>. Nature Methods <strong>21</strong>, <a href="https://doi.org/10.1038/s41592-024-02404-5">https://doi.org/10.1038/s41592-024-02404-5</a></p>
</div></div>



<p></p>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">1097</post-id>	</item>
		<item>
		<title>Camera shows relay race on DNA</title>
		<link>/2024/05/27/camera-shows-relay-race-on-dna/</link>
		
		<dc:creator><![CDATA[ARates]]></dc:creator>
		<pubDate>Mon, 27 May 2024 13:52:31 +0000</pubDate>
				<category><![CDATA[Press release]]></category>
		<guid isPermaLink="false">/?p=575</guid>

					<description><![CDATA[Figure by ©Simona Antonova. These little figures (transcription factors) jump up and down on symbolic piano keys on the DNA. They play short notes individually, but together they play a longer piece of music. This symbolizes transcription factors that activate DNA transcription for a very short time, and many of them simultaneously ensure that a [&#8230;]]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-columns alignwide is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<p class="has-small-font-size"><em>Figure by ©Simona Antonova. These little figures (transcription factors) jump up and down on symbolic piano keys on the DNA. They play short notes individually, but together they play a longer piece of music. This symbolizes transcription factors that activate DNA transcription for a very short time, and many of them simultaneously ensure that a gene can be read for a longer period.</em><br></p>



<p><strong>As <a href="https://www.sciencedirect.com/science/article/pii/S109727652400056X">new research</a> by <a href="https://www.nki.nl/research/research-groups/tineke-lenstra/">Tineke Lenstra and her colleagues</a> shows, our cells still contain many secrets. The researchers made an algorithm which allowed them to create incredibly detailed recordings of what happens inside a cell. And that fact didn’t escape the renowned microscopy company Zeiss. As it turns out, there’s somewhat of a relay race happening on our DNA, allowing cells to produce the necessary proteins.</strong></p>
</div>
</div>



<p>We know that our DNA contains information to create the proteins that our body needs. We know that DNA is comparable to a cookbook in this process, containing the recipes from which the cells select a particular recipe, then another, and so on. And we know that molecular kitchen assistants (transcription factors) open up this cookbook so the cooks can read the recipes (genes) and eventually produce new proteins. But these kitchen assistants usually work extremely short shifts: a few seconds each, while a recipe takes several minutes to be read. How is this possible?</p>



<h3 class="wp-block-heading">Mini shifts</h3>



<p><strong><a href="https://www.nki.nl/research/research-groups/tineke-lenstra/">Tineke Lenstra’s research group</a></strong> discovered that these kitchen assistants (transcription factors) perform somewhat of a relay race while opening the recipes. That allows them to let the DNA be read long enough, despite their brief shifts that last mere seconds. Their seamless teamwork allows the cell to create enough protein to function properly. Tineke and her colleagues have published their work <strong><a href="https://www.sciencedirect.com/science/article/pii/S109727652400056X">in Molecular Cell</a></strong>.</p>



<h3 class="wp-block-heading">Recording genes</h3>



<p>It’s quite unique that Tineke and her colleagues managed to study living cells in such great detail. The tricky part of these cells is that their contents are constantly in motion. Incredibly hard to record and study through a microscope.</p>



<p>That’s why they created an algorithm that allows the microscope to track one DNA recipe (gene) in living cells with great precision. Even when a gene is constantly on the move. The instructions in that algorithm can “pin” one particular point in the microscopy imaging, to allow it to track what happens to it. This creates many new opportunities to investigate cells in great detail.</p>



<p><a href="https://www.youtube-nocookie.com/embed/n7QZmM_DwIQ"></a></p>



<h3 class="wp-block-heading">License</h3>



<p>That fact didn’t escape renowned microscopy company Zeiss. This company licensed the algorithm developed by the NKI researchers so many other researchers can use it as well. Tineke: “It’s great that a method that we developed for a highly fundamental research question can be useful to various researchers from across all sorts of disciplines.”</p>
</div>
</div>



<div class="wp-block-group has-base-color has-light-blue-background-color has-text-color has-background has-link-color wp-elements-004c952b6c083342f6cb0d1d1eaad564" style="padding-top:var(--wp--preset--spacing--20);padding-right:var(--wp--preset--spacing--40);padding-bottom:var(--wp--preset--spacing--20);padding-left:var(--wp--preset--spacing--40)"><div class="wp-block-group__inner-container is-layout-constrained wp-container-core-group-is-layout-26f58b4f wp-block-group-is-layout-constrained">
<h4 class="wp-block-heading has-black-color has-text-color has-link-color wp-elements-cfc1229d547209fd523665ae8d1c3e35">More information:</h4>



<p>Read the full story at <a href="https://oncodeinstitute.nl/news/cinematography-of-transcription-real-time-tracking-software-reveals-relay-race-on-dna" data-type="link" data-id="https://oncodeinstitute.nl/news/cinematography-of-transcription-real-time-tracking-software-reveals-relay-race-on-dna">oncodeinstitute.nl</a> and <a href="https://www.nki.nl/news-events/news/camera-shows-relay-race-on-dna/" data-type="link" data-id="https://www.nki.nl/news-events/news/camera-shows-relay-race-on-dna/">nki.nl</a>!</p>



<p><em>This research was financially supported by KWF Dutch Cancer Society, Oncode Institute and the European Research Council.</em></p>



<p>Wim Pomp, Joseph V.W. Meeussen, Tineke L. Lenstra, (2024). Transcription factor exchange enables prolonged transcriptional bursts. <em>Molecular Cell</em>, <strong>84</strong>, <a href="https://doi.org/10.1016/j.molcel.2024.01.020">https://doi.org/10.1016/j.molcel.2024.01.020</a></p>
</div></div>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">575</post-id>	</item>
		<item>
		<title>Intelligent Microscopes: The Future of Biological Imaging</title>
		<link>/2024/03/19/intelligent-microscopes-the-future-of-biological-imaging/</link>
		
		<dc:creator><![CDATA[ARates]]></dc:creator>
		<pubDate>Tue, 19 Mar 2024 11:52:37 +0000</pubDate>
				<category><![CDATA[Press release]]></category>
		<guid isPermaLink="false">/?p=498</guid>

					<description><![CDATA[Instituto Gulbenkian de Ciência researchers have published a review highlighting the transformative potential of data-driven microscopy powered by machine learning. The article, titled &#8220;The rise of data-driven microscopy powered by machine learning,&#8221; appears in the latest issue of the Journal of Microscopy. The review, led by Leonor Morgado, explores how the integration of advanced computational [&#8230;]]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-columns alignwide is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<p>Instituto Gulbenkian de Ciência researchers have published a review highlighting the transformative potential of data-driven microscopy powered by machine learning. The article, titled &#8220;<a href="https://onlinelibrary.wiley.com/doi/full/10.1111/jmi.13282">The rise of data-driven microscopy powered by machine learning</a>,&#8221; appears in the latest issue of the Journal of Microscopy.</p>



<p>The review, led by Leonor Morgado, explores how the integration of advanced computational techniques, particularly machine learning, is revolutionizing the field of optical microscopy. By enabling microscopes to automatically adjust acquisition parameters based on real-time data analysis, these intelligent systems can optimize imaging conditions, enhance image quality, and extract meaningful information without heavy reliance on manual intervention.</p>



<p>The authors discuss various machine learning algorithms, such as support vector machines, convolutional neural networks, and generative adversarial networks, and their applications in microscopy image analysis tasks, including classification, segmentation, tracking, and reconstruction. They also highlight pioneering works and recent advances in integrating machine learning into microscopy acquisition workflows, demonstrating the ability to capture rare events, optimize illumination, switch modalities, and trigger targeted experiments.</p>



<p>&#8220;Data-driven microscopy represents a new era for optical imaging, overcoming inherent limitations through real-time feedback and automation,&#8221; said Leonor Morgado, the lead author of the review. &#8220;Intelligent microscopes have the potential to transform bioimaging by opening up new experimental possibilities and providing unprecedented spatiotemporal views into biological processes across scales.&#8221;</p>



<p>The review also discusses the remaining challenges and future outlook for data-driven microscopy, emphasizing the need for robust machine learning models, purpose-built instrumentation, user-friendly software, and extensive validation to ensure reproducibility and minimize bias.</p>



<p>As data-driven platforms become more accessible and ubiquitous, they are expected to empower researchers to image smarter, not just faster, fueling fundamental discoveries in cell and infection biology at the molecular level in physiological context.</p>
</div>
</div>



<div class="wp-block-group has-base-color has-light-blue-background-color has-text-color has-background has-link-color wp-elements-7d109dc2fd8fe045418fdc17c93d8f48" style="padding-top:var(--wp--preset--spacing--20);padding-right:var(--wp--preset--spacing--40);padding-bottom:var(--wp--preset--spacing--20);padding-left:var(--wp--preset--spacing--40)"><div class="wp-block-group__inner-container is-layout-constrained wp-container-core-group-is-layout-26f58b4f wp-block-group-is-layout-constrained">
<h4 class="wp-block-heading has-black-color has-text-color has-link-color wp-elements-cfc1229d547209fd523665ae8d1c3e35">More information:</h4>



<p>Written by <a href="mailto:lmorgado@igc.gulbenkian.pt">Leonor Morgado</a> and <a href="https://henriqueslab.org">The Optical Cell Biologists</a>.</p>



<p>Morgado, L., Gómez-de-Mariscal, E., Heil, H. S., &amp; Henriques, R. (2024). The rise of data-driven microscopy powered by machine learning. <em>Journal of Microscopy</em>, 1–8. <a href="https://doi.org/10.1111/jmi.13282">https://doi.org/10.1111/jmi.13282</a></p>
</div></div>
]]></content:encoded>
					
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">498</post-id>	</item>
	</channel>
</rss>
